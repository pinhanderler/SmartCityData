{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","from datetime import datetime, timezone\n","import uuid"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"4b098266-90a5-4f88-93d6-225645a75c84","normalized_state":"finished","queued_time":"2025-12-24T02:52:38.3263568Z","session_start_time":"2025-12-24T02:52:38.3273605Z","execution_start_time":"2025-12-24T02:52:49.6507304Z","execution_finish_time":"2025-12-24T02:52:51.0572248Z","parent_msg_id":"e7ec9e8d-9a8d-4ffa-b010-33f22d8b5154"},"text/plain":"StatementMeta(, 4b098266-90a5-4f88-93d6-225645a75c84, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef5f31bd-9eb7-40df-84da-e2c9b734c1ba"},{"cell_type":"code","source":["%run \"./nb_config\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"4b098266-90a5-4f88-93d6-225645a75c84","normalized_state":"finished","queued_time":"2025-12-24T02:52:39.2761363Z","session_start_time":null,"execution_start_time":"2025-12-24T02:52:51.5707815Z","execution_finish_time":"2025-12-24T02:52:51.6261219Z","parent_msg_id":"d30780a0-27ef-4dcd-823b-5ad482667961"},"text/plain":"StatementMeta(, 4b098266-90a5-4f88-93d6-225645a75c84, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Config Loaded\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d442fa63-b3bf-4519-b0fe-64dc51cebc37"},{"cell_type":"code","source":["# -------------------------------\n","# Spark session\n","# -------------------------------\n","spark = SparkSession.builder.getOrCreate()\n","\n","# -------------------------------\n","# PATH & RUN ID\n","# -------------------------------\n","LOG_PATH = f\"{BASE_PATH}/logs\"\n","RUN_ID = str(uuid.uuid4())\n","\n","# -------------------------------\n","# Helper\n","# -------------------------------\n","def get_timestamp():\n","    return datetime.now(timezone.utc)\n","\n","# -------------------------------\n","# Log schema\n","# -------------------------------\n","log_schema = StructType([\n","    StructField(\"timestamp\", TimestampType(), True),\n","    StructField(\"level\", StringType(), True),\n","    StructField(\"source\", StringType(), True),\n","    StructField(\"process\", StringType(), True),\n","    StructField(\"message\", StringType(), True),\n","    StructField(\"details\", StringType(), True),\n","    StructField(\"run_id\", StringType(), True)\n","])\n","\n","# -------------------------------\n","# Core logging function\n","# -------------------------------\n","def write_log(level, source, process, message, details=None):\n","\n","    log_data = [{\n","        \"timestamp\": get_timestamp(),\n","        \"level\": level,\n","        \"source\": source,\n","        \"process\": process,\n","        \"message\": message,\n","        \"details\": details,\n","        \"run_id\": RUN_ID\n","    }]\n","\n","    df = spark.createDataFrame(log_data, schema=log_schema)\n","    df.write.mode(\"append\").parquet(LOG_PATH)\n","\n","    print(f\"[{level}] {process} - {message}\")\n","\n","# -------------------------------\n","# Public log helpers\n","# -------------------------------\n","def log_info(source, process, message, details=None):\n","    write_log(\"INFO\", source, process, message, details)\n","\n","def log_warning(source, process, message, details=None):\n","    write_log(\"WARNING\", source, process, message, details)\n","\n","def log_error(source, process, message, details=None):\n","    write_log(\"ERROR\", source, process, message, details)\n","\n","def log_process_start(source, process):\n","    write_log(\"START\", source, process, \"Process started\")\n","\n","def log_process_end(source, process):\n","    write_log(\"END\", source, process, \"Process completed\")\n","\n","print(\"Logging System Loaded\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"4b098266-90a5-4f88-93d6-225645a75c84","normalized_state":"finished","queued_time":"2025-12-24T02:52:53.3568253Z","session_start_time":null,"execution_start_time":"2025-12-24T02:52:53.3581721Z","execution_finish_time":"2025-12-24T02:52:53.8695864Z","parent_msg_id":"28021a75-c5a2-4de3-80da-a6e58662fa61"},"text/plain":"StatementMeta(, 4b098266-90a5-4f88-93d6-225645a75c84, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Logging System Loaded\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11074fb7-78f5-4661-a7a9-957d4de377e5"},{"cell_type":"markdown","source":["# -------------------------------\n","# Logging Setup\n","# -------------------------------\n","LOG_PATH = f\"{BASE_PATH}/logs\"\n","RUN_ID = str(uuid.uuid4())\n","\n","def get_timestamp():\n","    return datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","log_schema = StructType([\n","    StructField(\"timestamp\", StringType(), True),\n","    StructField(\"level\", StringType(), True),\n","    StructField(\"source\", StringType(), True),\n","    StructField(\"process\", StringType(), True),\n","    StructField(\"message\", StringType(), True),\n","    StructField(\"details\", StringType(), True),\n","    StructField(\"run_id\", StringType(), True)\n","])\n","\n","def write_log(level, source, process, message, details=None):\n","    log_data = [{\n","        \"timestamp\": get_timestamp(),\n","        \"level\": level,\n","        \"source\": source,\n","        \"process\": process,\n","        \"message\": message,\n","        \"details\": str(details) if details else None,\n","        \"run_id\": RUN_ID\n","    }]\n","    df = spark.createDataFrame(log_data, schema=log_schema)\n","    df.write.mode(\"append\").parquet(LOG_PATH)\n","    print(f\"[{level}] {source} - {message}\")\n","\n","def log_process_start(source, process):\n","    write_log(\"START\", source, process, \"Process Started\")\n","\n","def log_process_end(source, process):\n","    write_log(\"END\", source, process, \"Process Completed\")\n","\n","def log_error(source, process, message, details=None):\n","    write_log(\"ERROR\", source, process, message, details)\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b228eb9c-0a14-4a6e-9785-67c9337fe9fb"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"4ed0e237-04d9-4f52-be5a-2fa5b9fb7c8a"}],"default_lakehouse":"4ed0e237-04d9-4f52-be5a-2fa5b9fb7c8a","default_lakehouse_name":"Project_Lakehouse","default_lakehouse_workspace_id":"e5c59b1a-f845-4467-b859-d65a059702db"}}},"nbformat":4,"nbformat_minor":5}